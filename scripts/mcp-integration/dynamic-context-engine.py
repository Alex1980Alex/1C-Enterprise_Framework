#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dynamic Context Engine v1.0
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤—ã–±–æ—Ä–∞ MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ 1C-Enterprise

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
–¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Å —É—á–µ—Ç–æ–º —Å–µ–º–∞–Ω—Ç–∏–∫–∏, —Ç–∏–ø–∞ —Ñ–∞–π–ª–æ–≤ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏.
"""

import json
import re
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum
import hashlib
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TaskComplexity(Enum):
    """–£—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á"""
    SIMPLE = "simple"           # –ü—Ä–æ—Å—Ç—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (—á—Ç–µ–Ω–∏–µ, –ø–æ–∏—Å–∫)
    MEDIUM = "medium"           # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –∑–∞–º–µ–Ω—ã
    COMPLEX = "complex"         # –ú–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π –∞–Ω–∞–ª–∏–∑, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏
    STRATEGIC = "strategic"     # –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è

class FileType(Enum):
    """–¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    BSL = "bsl"                 # 1C:Enterprise BSL –º–æ–¥—É–ª–∏
    METADATA = "metadata"       # XML –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    DOCUMENT = "document"       # PDF, DOCX, PPTX –¥–æ–∫—É–º–µ–Ω—Ç—ã
    WEB = "web"                # HTML, –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
    CONFIG = "config"          # –§–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    CODE = "code"              # –î—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
    TEXT = "text"              # –û–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã

class ToolCategory(Enum):
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
    MCP_SEMANTIC = "mcp_semantic"       # MCP –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    MCP_EXTERNAL = "mcp_external"       # MCP –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
    MCP_THINKING = "mcp_thinking"       # MCP –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    STANDARD = "standard"               # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

@dataclass
class ToolRecommendation:
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
    tool_name: str
    category: ToolCategory
    confidence: float           # 0.0 - 1.0
    reason: str
    parameters: Dict
    fallback_tool: Optional[str] = None
    estimated_time: Optional[str] = None

@dataclass
class TaskContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    user_request: str
    file_paths: List[str]
    file_types: List[FileType]
    complexity: TaskComplexity
    keywords: List[str]
    intent: str                 # –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    domain: str                 # –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å (1c, web, docs, etc.)

class DynamicContextEngine:
    """
    –î–≤–∏–∂–æ–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç
    –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å —É—á–µ—Ç–æ–º:
    - –¢–∏–ø–∞ —Ñ–∞–π–ª–æ–≤ –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    - –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏
    - –°–µ–º–∞–Ω—Ç–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
    - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    """

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "D:/1C-Enterprise_Framework/.claude/dynamic-context-config.json"
        self.rules_path = "D:/1C-Enterprise_Framework/.claude/mcp-priority-rules.md"
        self.cache_path = "D:/1C-Enterprise_Framework/cache/context-engine/"

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        Path(self.cache_path).mkdir(parents=True, exist_ok=True)
        self.load_configuration()
        self.load_tool_patterns()
        self.learning_data = self.load_learning_data()

    def load_configuration(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–≤–∏–∂–∫–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        except FileNotFoundError:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.config = self.create_default_config()
            self.save_configuration()

    def create_default_config(self) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "version": "1.0",
            "weights": {
                "file_type_match": 0.4,
                "semantic_match": 0.3,
                "complexity_match": 0.2,
                "learning_bonus": 0.1
            },
            "confidence_thresholds": {
                "high": 0.8,
                "medium": 0.6,
                "low": 0.4
            },
            "tool_availability": {
                "mcp__ast-grep-mcp__ast_grep": True,
                "mcp__serena__get_symbols_overview": True,
                "mcp__serena__find_symbol": True,
                "mcp__serena__find_referencing_symbols": True,
                "mcp__serena__replace_symbol_body": True,
                "mcp__serena__insert_after_symbol": True,
                "mcp__serena__search_for_pattern": True,
                "mcp__1c-framework-docs__search_docs": True,
                "mcp__universal-web-scraper__scrape_website": True,
                "mcp__docling__convert_document": True,
                "mcp__memory__create_entities": True,
                "mcp__sequential-thinking__sequentialthinking": True
            }
        }

    def load_tool_patterns(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤—ã–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø—Ä–∞–≤–∏–ª –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏"""
        self.tool_patterns = {
            # BSL –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û AST-grep
            "bsl_structure_analysis": {
                "patterns": ["–æ–±–∑–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ bsl", "—Å–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π", "—Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Ü–µ–¥—É—Ä", "–∞–Ω–∞–ª–∏–∑ –º–æ–¥—É–ª—è"],
                "file_types": [FileType.BSL],
                "primary_tool": "mcp__ast-grep-mcp__ast_grep",
                "fallback_tool": "mcp__serena__get_symbols_overview",
                "confidence_boost": 0.3,
                "parameters": {
                    "pattern": "–§—É–Ω–∫—Ü–∏—è $NAME($$$ARGS)",
                    "bsl_type": "functions"
                }
            },

            # –ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏/–ø—Ä–æ—Ü–µ–¥—É—Ä—ã
            "bsl_function_search": {
                "patterns": ["–Ω–∞–π—Ç–∏ —Ñ—É–Ω–∫—Ü–∏—é", "–Ω–∞–π—Ç–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä—É", "–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏"],
                "file_types": [FileType.BSL],
                "primary_tool": "mcp__ast-grep-mcp__ast_grep",
                "fallback_tool": "mcp__serena__find_symbol",
                "confidence_boost": 0.25,
                "parameters": {
                    "bsl_type": "auto",
                    "include_body": True
                }
            },

            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            "dependency_analysis": {
                "patterns": ["–≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è", "–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏", "—Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é", "–≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–∏"],
                "file_types": [FileType.BSL],
                "primary_tool": "mcp__serena__find_referencing_symbols",
                "fallback_tool": "Grep",
                "confidence_boost": 0.2
            },

            # –ó–∞–º–µ–Ω–∞ –∫–æ–¥–∞
            "code_replacement": {
                "patterns": ["–∑–∞–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é", "–∏–∑–º–µ–Ω–∏—Ç—å –ø—Ä–æ—Ü–µ–¥—É—Ä—É", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥", "–æ–±–Ω–æ–≤–∏—Ç—å –∫–æ–¥"],
                "file_types": [FileType.BSL],
                "primary_tool": "mcp__serena__replace_symbol_body",
                "fallback_tool": "Edit",
                "confidence_boost": 0.2
            },

            # –ü–æ–∏—Å–∫ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            "documentation_search": {
                "patterns": ["–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å", "–ø—Ä–∏–º–µ—Ä—ã", "—Ñ—Ä–µ–π–º–≤–æ—Ä–∫", "best practices"],
                "file_types": [],
                "primary_tool": "mcp__1c-framework-docs__search_docs",
                "fallback_tool": None,
                "confidence_boost": 0.25,
                "parameters": {
                    "search_type": "hybrid",
                    "limit": 5
                }
            },

            # –ü–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å–∞–π—Ç–æ–≤
            "web_scraping": {
                "patterns": ["–ø–∞—Ä—Å–∏–Ω–≥", "its.1c.ru", "–≤–µ–±-—Å–∞–π—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å —Å–∞–π—Ç–∞", "–∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ"],
                "file_types": [FileType.WEB],
                "primary_tool": "mcp__universal-web-scraper__scrape_website",
                "fallback_tool": None,
                "confidence_boost": 0.3,
                "parameters": {
                    "adapter_type": "auto",
                    "include_links": True,
                    "save_to_memory": True
                }
            },

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            "document_conversion": {
                "patterns": ["–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å pdf", "docx –≤ markdown", "–∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç", "ocr"],
                "file_types": [FileType.DOCUMENT],
                "primary_tool": "mcp__docling__convert_document",
                "fallback_tool": None,
                "confidence_boost": 0.35,
                "parameters": {
                    "extract_images": True,
                    "ocr_enabled": True
                }
            },

            # –°–ª–æ–∂–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            "complex_analysis": {
                "patterns": ["–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å", "—Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π"],
                "file_types": [],
                "primary_tool": "mcp__sequential-thinking__sequentialthinking",
                "fallback_tool": None,
                "confidence_boost": 0.2,
                "parameters": {
                    "totalThoughts": 5,
                    "nextThoughtNeeded": True
                }
            },

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π
            "knowledge_storage": {
                "patterns": ["–∑–∞–ø–æ–º–Ω–∏—Ç—å", "—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", "knowledge graph", "–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å", "–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π"],
                "file_types": [],
                "primary_tool": "mcp__memory__create_entities",
                "fallback_tool": "Write",
                "confidence_boost": 0.15
            }
        }

    def analyze_request(self, user_request: str, file_paths: List[str] = None) -> TaskContext:
        """
        –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            user_request: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)

        Returns:
            TaskContext: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º
        """
        file_paths = file_paths or []

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
        file_types = self._detect_file_types(file_paths, user_request)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self._extract_keywords(user_request)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexity = self._determine_complexity(user_request, keywords)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        intent = self._determine_intent(user_request, keywords)

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
        domain = self._determine_domain(user_request, file_types)

        return TaskContext(
            user_request=user_request,
            file_paths=file_paths,
            file_types=file_types,
            complexity=complexity,
            keywords=keywords,
            intent=intent,
            domain=domain
        )

    def recommend_tools(self, context: TaskContext) -> List[ToolRecommendation]:
        """
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏

        Returns:
            List[ToolRecommendation]: –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []

        # –ê–Ω–∞–ª–∏–∑ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern_name, pattern_config in self.tool_patterns.items():
            confidence = self._calculate_confidence(context, pattern_config)

            if confidence > self.config["confidence_thresholds"]["low"]:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
                primary_tool = pattern_config["primary_tool"]
                if not self.config["tool_availability"].get(primary_tool, False):
                    continue

                # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                parameters = pattern_config.get("parameters", {})
                if context.file_paths:
                    parameters["path"] = context.file_paths[0]

                # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                estimated_time = self._estimate_execution_time(primary_tool, context.complexity)

                recommendation = ToolRecommendation(
                    tool_name=primary_tool,
                    category=self._get_tool_category(primary_tool),
                    confidence=confidence,
                    reason=f"–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º '{pattern_name}'",
                    parameters=parameters,
                    fallback_tool=pattern_config.get("fallback_tool"),
                    estimated_time=estimated_time
                )

                recommendations.append(recommendation)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        recommendations.sort(key=lambda x: x.confidence, reverse=True)

        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±–æ—Ä–∞
        self._update_learning_data(context, recommendations)

        return recommendations[:5]  # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

    def _detect_file_types(self, file_paths: List[str], user_request: str) -> List[FileType]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        file_types = []

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤
        for path in file_paths:
            path_lower = path.lower()
            if path_lower.endswith('.bsl'):
                file_types.append(FileType.BSL)
            elif path_lower.endswith(('.pdf', '.docx', '.doc', '.pptx')):
                file_types.append(FileType.DOCUMENT)
            elif path_lower.endswith(('.xml',)) and 'metadata' in path_lower:
                file_types.append(FileType.METADATA)
            elif path_lower.endswith(('.html', '.htm')):
                file_types.append(FileType.WEB)
            elif path_lower.endswith(('.json', '.yaml', '.yml', '.ini')):
                file_types.append(FileType.CONFIG)
            else:
                file_types.append(FileType.CODE)

        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        request_lower = user_request.lower()
        if any(word in request_lower for word in ['bsl', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '—Ñ—É–Ω–∫—Ü–∏—è', '1—Å']):
            file_types.append(FileType.BSL)
        if any(word in request_lower for word in ['pdf', 'docx', '–¥–æ–∫—É–º–µ–Ω—Ç']):
            file_types.append(FileType.DOCUMENT)
        if any(word in request_lower for word in ['—Å–∞–π—Ç', '–≤–µ–±', 'url', 'http']):
            file_types.append(FileType.WEB)

        return list(set(file_types))

    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –í–∞–∂–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è 1C –∏ MCP
        important_keywords = [
            '—Ñ—É–Ω–∫—Ü–∏—è', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '–º–æ–¥—É–ª—å', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ',
            '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫', '—Ä–µ–≥–∏—Å—Ç—Ä', '–æ—Ç—á–µ—Ç', '–æ–±—Ä–∞–±–æ—Ç–∫–∞',
            '–∞–Ω–∞–ª–∏–∑', '–ø–æ–∏—Å–∫', '–∑–∞–º–µ–Ω–∞', '—Å–æ–∑–¥–∞–Ω–∏–µ', '—É–¥–∞–ª–µ–Ω–∏–µ',
            '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞', '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏', '—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è',
            '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '–ø–∞—Ä—Å–∏–Ω–≥', '–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è', '—ç–∫—Å–ø–æ—Ä—Ç'
        ]

        text_lower = text.lower()
        found_keywords = [kw for kw in important_keywords if kw in text_lower]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ regex
        additional_patterns = [
            r'\b(–Ω–∞–π—Ç–∏|–Ω–∞–π–¥–∏)\b',
            r'\b(—Å–æ–∑–¥–∞—Ç—å|—Å–æ–∑–¥–∞–π)\b',
            r'\b(–∑–∞–º–µ–Ω–∏—Ç—å|–∑–∞–º–µ–Ω–∏)\b',
            r'\b(–∞–Ω–∞–ª–∏–∑|–ø—Ä–æ–∞–Ω–∞–ª–∏–∑)\w*',
            r'\b(–¥–æ–∫—É–º–µ–Ω—Ç|–¥–æ–∫—É–º)\w*'
        ]

        for pattern in additional_patterns:
            matches = re.findall(pattern, text_lower)
            found_keywords.extend(matches)

        return list(set(found_keywords))

    def _determine_complexity(self, request: str, keywords: List[str]) -> TaskComplexity:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏"""
        request_lower = request.lower()

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏
        strategic_indicators = ['–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', '–º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è', '–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ']
        if any(indicator in request_lower for indicator in strategic_indicators):
            return TaskComplexity.STRATEGIC

        # –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏
        complex_indicators = ['–∞–Ω–∞–ª–∏–∑', '—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏', '–≥—Ä–∞—Ñ', '–º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã–π']
        if any(indicator in request_lower for indicator in complex_indicators):
            return TaskComplexity.COMPLEX

        # –°—Ä–µ–¥–Ω–∏–µ –∑–∞–¥–∞—á–∏
        medium_indicators = ['–∑–∞–º–µ–Ω–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '–æ–±–Ω–æ–≤–∏—Ç—å', '–¥–æ–±–∞–≤–∏—Ç—å', '—Å–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é']
        if any(indicator in request_lower for indicator in medium_indicators):
            return TaskComplexity.MEDIUM

        # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return TaskComplexity.SIMPLE

    def _determine_intent(self, request: str, keywords: List[str]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        request_lower = request.lower()

        intent_patterns = {
            'search': ['–Ω–∞–π—Ç–∏', '–ø–æ–∏—Å–∫', '–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è', '–∏—Å–∫–∞—Ç—å'],
            'analyze': ['–∞–Ω–∞–ª–∏–∑', '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑', '–∏–∑—É—á–∏—Ç—å', '–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å'],
            'modify': ['–∑–∞–º–µ–Ω–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '–æ–±–Ω–æ–≤–∏—Ç—å', '–∏—Å–ø—Ä–∞–≤–∏—Ç—å'],
            'create': ['—Å–æ–∑–¥–∞—Ç—å', '–¥–æ–±–∞–≤–∏—Ç—å', '–Ω–æ–≤—ã–π', '—Å–¥–µ–ª–∞—Ç—å'],
            'document': ['–¥–æ–∫—É–º–µ–Ω—Ç', '–∑–∞–¥–æ–∫—É–º–µ–Ω—Ç', '–æ–ø–∏—Å–∞—Ç—å', '–∑–∞–ø–æ–º–Ω–∏—Ç—å'],
            'convert': ['–∫–æ–Ω–≤–µ—Ä—Ç', '–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å', '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç']
        }

        for intent, patterns in intent_patterns.items():
            if any(pattern in request_lower for pattern in patterns):
                return intent

        return 'unknown'

    def _determine_domain(self, request: str, file_types: List[FileType]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏"""
        request_lower = request.lower()

        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
        if FileType.BSL in file_types:
            return '1c'
        if FileType.DOCUMENT in file_types:
            return 'documents'
        if FileType.WEB in file_types:
            return 'web'

        # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        if any(word in request_lower for word in ['1—Å', 'bsl', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ']):
            return '1c'
        if any(word in request_lower for word in ['–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '—Ñ—Ä–µ–π–º–≤–æ—Ä–∫', 'its.1c']):
            return 'documentation'
        if any(word in request_lower for word in ['—Å–∞–π—Ç', '–≤–µ–±', '–ø–∞—Ä—Å–∏–Ω–≥']):
            return 'web'

        return 'general'

    def _calculate_confidence(self, context: TaskContext, pattern_config: Dict) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        confidence = 0.0
        weights = self.config["weights"]

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        if pattern_config["file_types"]:
            file_type_match = len(set(context.file_types) & set(pattern_config["file_types"])) / len(pattern_config["file_types"])
            confidence += file_type_match * weights["file_type_match"]
        else:
            confidence += 0.1  # –ë–∞–∑–æ–≤—ã–π –±–æ–Ω—É—Å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º —Ç–µ–∫—Å—Ç–∞
        request_lower = context.user_request.lower()
        pattern_matches = sum(1 for pattern in pattern_config["patterns"] if pattern in request_lower)
        if pattern_matches > 0:
            semantic_match = min(pattern_matches / len(pattern_config["patterns"]), 1.0)
            confidence += semantic_match * weights["semantic_match"]

        # –ë–æ–Ω—É—Å –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        complexity_bonus = pattern_config.get("confidence_boost", 0.0)
        confidence += complexity_bonus * weights["complexity_match"]

        # –ë–æ–Ω—É—Å –æ—Ç –æ–±—É—á–µ–Ω–∏—è
        learning_bonus = self._get_learning_bonus(context, pattern_config["primary_tool"])
        confidence += learning_bonus * weights["learning_bonus"]

        return min(confidence, 1.0)

    def _get_tool_category(self, tool_name: str) -> ToolCategory:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        if tool_name.startswith('mcp__'):
            if 'serena' in tool_name or 'ast-grep' in tool_name:
                return ToolCategory.MCP_SEMANTIC
            elif 'sequential-thinking' in tool_name:
                return ToolCategory.MCP_THINKING
            else:
                return ToolCategory.MCP_EXTERNAL
        return ToolCategory.STANDARD

    def _estimate_execution_time(self, tool_name: str, complexity: TaskComplexity) -> str:
        """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        base_times = {
            'mcp__ast-grep-mcp__ast_grep': '5-15 —Å–µ–∫',
            'mcp__serena__get_symbols_overview': '10-30 —Å–µ–∫',
            'mcp__serena__find_symbol': '5-20 —Å–µ–∫',
            'mcp__serena__find_referencing_symbols': '15-45 —Å–µ–∫',
            'mcp__universal-web-scraper__scrape_website': '30 —Å–µ–∫ - 3 –º–∏–Ω',
            'mcp__docling__convert_document': '30 —Å–µ–∫ - 5 –º–∏–Ω',
            'mcp__sequential-thinking__sequentialthinking': '2-10 –º–∏–Ω'
        }

        base_time = base_times.get(tool_name, '10-30 —Å–µ–∫')

        if complexity in [TaskComplexity.COMPLEX, TaskComplexity.STRATEGIC]:
            return f"{base_time} (—É–≤–µ–ª–∏—á–µ–Ω–æ –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏)"

        return base_time

    def _get_learning_bonus(self, context: TaskContext, tool_name: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–æ–Ω—É—Å–∞ –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        request_hash = hashlib.md5(context.user_request.encode()).hexdigest()

        if request_hash in self.learning_data:
            successful_tools = self.learning_data[request_hash].get('successful_tools', [])
            if tool_name in successful_tools:
                return 0.1  # 10% –±–æ–Ω—É—Å –∑–∞ —É—Å–ø–µ—à–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

        return 0.0

    def _update_learning_data(self, context: TaskContext, recommendations: List[ToolRecommendation]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        request_hash = hashlib.md5(context.user_request.encode()).hexdigest()

        if request_hash not in self.learning_data:
            self.learning_data[request_hash] = {
                'request': context.user_request,
                'timestamp': datetime.now().isoformat(),
                'recommendations': [],
                'successful_tools': []
            }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        self.learning_data[request_hash]['recommendations'] = [
            {
                'tool': rec.tool_name,
                'confidence': rec.confidence,
                'reason': rec.reason
            }
            for rec in recommendations[:3]
        ]

        self.save_learning_data()

    def load_learning_data(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        learning_file = Path(self.cache_path) / "learning_data.json"
        try:
            if learning_file.exists():
                with open(learning_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è: {e}")

        return {}

    def save_learning_data(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        learning_file = Path(self.cache_path) / "learning_data.json"
        try:
            with open(learning_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è: {e}")

    def save_configuration(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")

    def generate_recommendation_report(self, context: TaskContext, recommendations: List[ToolRecommendation]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
        report = f"""
# Dynamic Context Engine - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º

## –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞
**–ó–∞–ø—Ä–æ—Å:** {context.user_request}
**–§–∞–π–ª—ã:** {', '.join(context.file_paths) if context.file_paths else '–ù–µ —É–∫–∞–∑–∞–Ω—ã'}
**–¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤:** {', '.join([ft.value for ft in context.file_types])}
**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {context.complexity.value}
**–ù–∞–º–µ—Ä–µ–Ω–∏–µ:** {context.intent}
**–ü—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å:** {context.domain}
**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(context.keywords)}

## –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

"""

        for i, rec in enumerate(recommendations, 1):
            confidence_level = "–í—ã—Å–æ–∫–∞—è" if rec.confidence >= 0.8 else "–°—Ä–µ–¥–Ω—è—è" if rec.confidence >= 0.6 else "–ù–∏–∑–∫–∞—è"

            report += f"""
### {i}. {rec.tool_name}
**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {rec.confidence:.2f} ({confidence_level})
**–ü—Ä–∏—á–∏–Ω–∞:** {rec.reason}
**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {rec.category.value}
**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {rec.estimated_time or '–ù–µ –æ—Ü–µ–Ω–µ–Ω–æ'}
**Fallback:** {rec.fallback_tool or '–ù–µ—Ç'}

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
```json
{json.dumps(rec.parameters, ensure_ascii=False, indent=2)}
```
"""

        report += f"""
## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
**–í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:** {len(self.tool_patterns)}
**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**–í–µ—Ä—Å–∏—è –¥–≤–∏–∂–∫–∞:** 1.0
"""

        return report

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Dynamic Context Engine"""
    engine = DynamicContextEngine()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_cases = [
        {
            "request": "–ù–∞–π–¥–∏ –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –º–æ–¥—É–ª–µ ObjectModule.bsl",
            "files": ["src/DataProcessors/–≥–∫—Å_–ê–†–ú–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π–ö–æ–º–ø–æ–∑–∏—Ç/Ext/ObjectModule.bsl"]
        },
        {
            "request": "–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –ó–∞–ø–æ–ª–Ω–∏—Ç—å–°–ø–∏—Å–æ–∫–ü—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π–ö–æ–º–ø–æ–∑–∏—Ç?",
            "files": ["Forms/–§–æ—Ä–º–∞/Ext/Form/Module.bsl"]
        },
        {
            "request": "–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Å–∞–π—Ç–∞ its.1c.ru",
            "files": []
        },
        {
            "request": "–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –≤ Markdown",
            "files": ["docs/technical_specification.pdf"]
        },
        {
            "request": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏—è",
            "files": ["src/projects/configuration/"]
        }
    ]

    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Dynamic Context Engine v1.0\n")

    for i, test_case in enumerate(test_cases, 1):
        print(f"{'='*60}")
        print(f"–¢–ï–°–¢ {i}: {test_case['request']}")
        print(f"{'='*60}")

        # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = engine.analyze_request(test_case['request'], test_case['files'])

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = engine.recommend_tools(context)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = engine.generate_recommendation_report(context, recommendations)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_file = Path(engine.cache_path) / f"test_report_{i}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")

        # –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ —Ç–æ–ø-3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print("\nüéØ –¢–û–ü-3 –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for j, rec in enumerate(recommendations[:3], 1):
            print(f"{j}. {rec.tool_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {rec.confidence:.2f})")
            print(f"   –ü—Ä–∏—á–∏–Ω–∞: {rec.reason}")

        print("\n")

    print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã! –û—Ç—á–µ—Ç—ã –≤ –ø–∞–ø–∫–µ cache/context-engine/")

if __name__ == "__main__":
    main()