#!/usr/bin/env python3
"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ 1C
–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ (SQLite FTS5) –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (sentence-transformers)
"""

import json
import sqlite3
import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
from datetime import datetime

try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False
    print("–í–Ω–∏–º–∞–Ω–∏–µ: sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

@dataclass
class Document:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    id: str
    title: str
    path: str
    content: str
    content_preview: str
    size: int
    modified: str
    tags: List[str]
    doc_type: str

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    document: Document
    score: float
    match_type: str  # 'fulltext', 'semantic', 'hybrid'
    snippet: str

class HybridSearchEngine:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫"""
    
    def __init__(self, db_path: str = "cache/docs-mcp/hybrid_search.db"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
        
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î: {self.db_path}")
        
        self.embedding_model = None
        if EMBEDDINGS_AVAILABLE:
            try:
                # –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ
                self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                self.logger.info("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: paraphrase-multilingual-MiniLM-L12-v2")
            except Exception as e:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        else:
            self.logger.warning("sentence-transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω.")
        
        try:
            self._init_database()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
            raise
        
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                path TEXT UNIQUE NOT NULL,
                content TEXT NOT NULL,
                content_preview TEXT,
                size INTEGER,
                modified TEXT,
                tags TEXT,  -- JSON array
                doc_type TEXT,
                content_hash TEXT
            )
        """)
        
        # FTS5 —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                id UNINDEXED,
                title,
                content,
                tags,
                content='documents',
                content_rowid='rowid'
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
        if self.embedding_model:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS embeddings (
                    document_id TEXT PRIMARY KEY,
                    embedding BLOB,
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            """)
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_docs_type ON documents(doc_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_docs_modified ON documents(modified)")
        
        conn.commit()
        conn.close()
        
        print(f"[OK] –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.db_path}")
    
    def _generate_doc_id(self, path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        return hashlib.md5(path.encode()).hexdigest()[:12]
    
    def _get_content_hash(self, content: str) -> str:
        """–•–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        return hashlib.sha256(content.encode()).hexdigest()
    
    def index_document(self, file_path: str, doc_type: str = "markdown") -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        path = Path(file_path)
        
        if not path.exists():
            print(f"[ERROR] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return False
        
        try:
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            doc_id = self._generate_doc_id(str(path))
            content_hash = self._get_content_hash(content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å
            if self._is_document_current(doc_id, content_hash):
                print(f"‚è≠Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –∞–∫—Ç—É–∞–ª–µ–Ω: {path.name}")
                return True
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            document = Document(
                id=doc_id,
                title=path.stem,
                path=str(path),
                content=content,
                content_preview=content[:500] + "..." if len(content) > 500 else content,
                size=len(content),
                modified=datetime.fromtimestamp(path.stat().st_mtime).isoformat(),
                tags=self._extract_tags(content),
                doc_type=doc_type
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
            self._save_document(document, content_hash)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
            if self.embedding_model:
                self._generate_embedding(document)
            
            print(f"[OK] –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω: {path.name}")
            return True

        except Exception as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {file_path}: {e}")
            return False
    
    def _is_document_current(self, doc_id: str, content_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT content_hash FROM documents WHERE id = ?", 
            (doc_id,)
        )
        result = cursor.fetchone()
        conn.close()
        
        return result and result[0] == content_hash
    
    def _extract_tags(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        tags = []
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ç–µ–≥–æ–≤
        if "Task Master" in content:
            tags.append("task-master")
        if "BSL" in content:
            tags.append("bsl")
        if "MCP" in content:
            tags.append("mcp")
        if "Claude" in content:
            tags.append("claude")
        if "1C" in content or "1–°" in content:
            tags.append("1c")
        if "API" in content:
            tags.append("api")
        if "integration" in content.lower() or "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è" in content.lower():
            tags.append("integration")
            
        return tags
    
    def _save_document(self, document: Document, content_hash: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –±–∞–∑—É"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        cursor.execute("""
            INSERT OR REPLACE INTO documents 
            (id, title, path, content, content_preview, size, modified, tags, doc_type, content_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            document.id, document.title, document.path, document.content,
            document.content_preview, document.size, document.modified,
            json.dumps(document.tags), document.doc_type, content_hash
        ))
        
        # FTS5 —Ç–∞–±–ª–∏—Ü–∞
        cursor.execute("""
            INSERT OR REPLACE INTO documents_fts (id, title, content, tags)
            VALUES (?, ?, ?, ?)
        """, (
            document.id, document.title, document.content, 
            " ".join(document.tags)
        ))
        
        conn.commit()
        conn.close()
    
    def _generate_embedding(self, document: Document):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not self.embedding_model:
            return
        
        try:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            text_to_embed = f"{document.title} {document.content}"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.embedding_model.encode(text_to_embed)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO embeddings (document_id, embedding)
                VALUES (?, ?)
            """, (document.id, embedding.tobytes()))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"[WARNING] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è {document.title}: {e}")
    
    def search(self, query: str, limit: int = 10, search_type: str = "hybrid") -> List[SearchResult]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ ('fulltext', 'semantic', 'hybrid')
        """
        if search_type == "fulltext":
            return self._fulltext_search(query, limit)
        elif search_type == "semantic" and self.embedding_model:
            return self._semantic_search(query, limit)
        elif search_type == "hybrid":
            return self._hybrid_search(query, limit)
        else:
            # Fallback –Ω–∞ –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            return self._fulltext_search(query, limit)
    
    def _fulltext_search(self, query: str, limit: int) -> List[SearchResult]:
        """–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ FTS5"""
        try:
            self.logger.debug(f"–ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: '{query}', –ª–∏–º–∏—Ç: {limit}")
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # FTS5 –∑–∞–ø—Ä–æ—Å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º bm25
            cursor.execute("""
                SELECT d.*, bm25(documents_fts) as score
                FROM documents_fts
                JOIN documents d ON documents_fts.rowid = d.rowid
                WHERE documents_fts MATCH ?
                ORDER BY score
                LIMIT ?
            """, (query, limit))
            
            results = []
            for row in cursor.fetchall():
                try:
                    document = self._row_to_document(row[:-1])  # –ò—Å–∫–ª—é—á–∞–µ–º score
                    score = row[-1]
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º snippet
                    snippet = self._generate_snippet(document.content, query)
                    
                    results.append(SearchResult(
                        document=document,
                        score=abs(score),  # BM25 –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º
                        match_type="fulltext",
                        snippet=snippet
                    ))
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                    continue
            
            conn.close()
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (fulltext)")
            return results
            
        except sqlite3.Error as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ SQL –≤ fulltext_search: {e}")
            return []
        except Exception as e:
            self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ fulltext_search: {e}")
            return []
    
    def _semantic_search(self, query: str, limit: int) -> List[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        if not self.embedding_model:
            self.logger.warning("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []
        
        try:
            self.logger.debug(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: '{query}', –ª–∏–º–∏—Ç: {limit}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_model.encode(query)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –±–∞–∑—ã
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT e.document_id, e.embedding, d.*
                FROM embeddings e
                JOIN documents d ON e.document_id = d.id
            """)
            
            results = []
            for row in cursor.fetchall():
                try:
                    doc_id = row[0]
                    stored_embedding = np.frombuffer(row[1], dtype=np.float32)
                    document = self._row_to_document(row[2:])
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarity = np.dot(query_embedding, stored_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(stored_embedding)
                    )
                    
                    snippet = self._generate_snippet(document.content, query)
                    
                    results.append(SearchResult(
                        document=document,
                        score=float(similarity),
                        match_type="semantic",
                        snippet=snippet
                    ))
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {row[0] if row else 'unknown'}: {e}")
                    continue
            
            conn.close()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
            results.sort(key=lambda x: x.score, reverse=True)
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (semantic)")
            return results[:limit]
            
        except sqlite3.Error as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ SQL –≤ semantic_search: {e}")
            return []
        except Exception as e:
            self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ semantic_search: {e}")
            return []
    
    def _hybrid_search(self, query: str, limit: int) -> List[SearchResult]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è fulltext + semantic)"""
        try:
            self.logger.debug(f"–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: '{query}', –ª–∏–º–∏—Ç: {limit}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤
            fulltext_results = self._fulltext_search(query, limit * 2)
            semantic_results = self._semantic_search(query, limit * 2) if self.embedding_model else []
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ document.id
            combined = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in fulltext_results:
                doc_id = result.document.id
                combined[doc_id] = result
                combined[doc_id].score = result.score * 0.7  # –í–µ—Å 70%
            
            # –î–æ–±–∞–≤–ª—è–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in semantic_results:
                doc_id = result.document.id
                if doc_id in combined:
                    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å–∫–æ—Ä—ã
                    combined[doc_id].score += result.score * 0.3  # –í–µ—Å 30%
                    combined[doc_id].match_type = "hybrid"
                else:
                    combined[doc_id] = result
                    combined[doc_id].score = result.score * 0.3
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = list(combined.values())
            final_results.sort(key=lambda x: x.score, reverse=True)
            
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(final_results[:limit])} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (hybrid)")
            return final_results[:limit]
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ hybrid_search: {e}")
            # Fallback –Ω–∞ fulltext –ø–æ–∏—Å–∫
            self.logger.info("–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ fulltext –ø–æ–∏—Å–∫ –∫–∞–∫ fallback")
            return self._fulltext_search(query, limit)
    
    def _row_to_document(self, row) -> Document:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ë–î –≤ –æ–±—ä–µ–∫—Ç Document"""
        return Document(
            id=row[0],
            title=row[1],
            path=row[2],
            content=row[3],
            content_preview=row[4],
            size=row[5],
            modified=row[6],
            tags=json.loads(row[7]) if row[7] else [],
            doc_type=row[8]
        )
    
    def _generate_snippet(self, content: str, query: str, max_length: int = 200) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–Ω–∏–ø–ø–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        content_lower = content.lower()
        
        # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        pos = content_lower.find(query_lower)
        
        if pos == -1:
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –±–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ
            return content[:max_length] + "..." if len(content) > max_length else content
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–Ω–∏–ø–ø–µ—Ç–∞
        start = max(0, pos - max_length // 3)
        end = min(len(content), pos + len(query) + max_length // 3)
        
        snippet = content[start:end]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."
            
        return snippet
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute("SELECT COUNT(*) FROM documents")
        total_docs = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM embeddings")
        docs_with_embeddings = cursor.fetchone()[0]
        
        cursor.execute("SELECT SUM(size) FROM documents")
        total_size = cursor.fetchone()[0] or 0
        
        cursor.execute("SELECT doc_type, COUNT(*) FROM documents GROUP BY doc_type")
        types_stats = dict(cursor.fetchall())
        
        cursor.execute("""
            SELECT COUNT(DISTINCT tags.value) 
            FROM documents, json_each(documents.tags) AS tags
        """)
        unique_tags = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "total_documents": total_docs,
            "documents_with_embeddings": docs_with_embeddings,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "document_types": types_stats,
            "unique_tags": unique_tags,
            "embeddings_enabled": EMBEDDINGS_AVAILABLE and self.embedding_model is not None,
            "model_name": "paraphrase-multilingual-MiniLM-L12-v2" if self.embedding_model else None
        }


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    engine = HybridSearchEngine()
    
    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
    docs_path = Path("–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É")
    
    if docs_path.exists():
        print(f"üìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {docs_path}")
        
        for md_file in docs_path.rglob("*.md"):
            engine.index_document(str(md_file))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = engine.get_statistics()
    print(f"\n[STATS] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞:")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
    print(f"   –° —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {stats['documents_with_embeddings']}")
    print(f"   –†–∞–∑–º–µ—Ä: {stats['total_size_mb']} MB")
    print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {'[OK]' if stats['embeddings_enabled'] else '[NO]'}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "Task Master",
        "BSL –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞",
        "MCP —Å–µ—Ä–≤–µ—Ä",
        "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Claude"
    ]
    
    for query in test_queries:
        print(f"\n[SEARCH] –ü–æ–∏—Å–∫: '{query}'")
        results = engine.search(query, limit=3)
        
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result.document.title}")
            print(f"     –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result.score:.3f} ({result.match_type})")
            print(f"     –§–∞–π–ª: {Path(result.document.path).name}")


if __name__ == "__main__":
    main()