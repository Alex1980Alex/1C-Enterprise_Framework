"""
BSL Async Indexer - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è BSL —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
–í–µ—Ä—Å–∏—è: 2.0 —Å async/batch processing –¥–ª—è Week 2, Day 3

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ (asyncio)
- Batch processing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- Progress monitoring —Å real-time –æ—Ç—á–µ—Ç–∞–º–∏
- Error handling –∏ retry logic
- –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""

import os
import sys
import json
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import time

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from services.embedding_service import EmbeddingService
from utils.bsl_parser import BSLParser, BSLModule

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_dir = Path(__file__).parent.parent.parent / "logs"
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'indexing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class IndexedFile:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    file_path: str
    module_type: str
    functions_count: int
    variables_count: int
    searchable_text: str
    embedding: List[float]
    indexed_at: str
    file_size: int
    processing_time_ms: float = 0.0


@dataclass
class IndexingProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    total_files: int = 0
    processed_files: int = 0
    successful: int = 0
    failed: int = 0
    skipped: int = 0
    start_time: float = 0.0

    @property
    def progress_percent(self) -> float:
        if self.total_files == 0:
            return 0.0
        return (self.processed_files / self.total_files) * 100

    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time

    @property
    def files_per_second(self) -> float:
        if self.elapsed_time == 0:
            return 0.0
        return self.processed_files / self.elapsed_time

    @property
    def estimated_remaining_seconds(self) -> float:
        if self.files_per_second == 0:
            return 0.0
        remaining_files = self.total_files - self.processed_files
        return remaining_files / self.files_per_second


class AsyncBSLIndexer:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä BSL —Ñ–∞–π–ª–æ–≤ —Å batch processing
    """

    def __init__(
        self,
        output_dir: str = "D:/1C-Enterprise_Framework/ai-memory-system/data/index",
        embedding_model: str = "nomic-embed-text:latest",
        batch_size: int = 10,
        max_workers: int = 4,
        retry_attempts: int = 3
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞

        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            batch_size: –†–∞–∑–º–µ—Ä batch –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ worker threads
            retry_attempts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.embedding_service = EmbeddingService(model=embedding_model)
        self.parser = BSLParser()

        self.batch_size = batch_size
        self.max_workers = max_workers
        self.retry_attempts = retry_attempts

        self.indexed_files: List[IndexedFile] = []
        self.progress = IndexingProgress()

        # –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏ –¥–ª—è retry
        self.failed_files: List[str] = []

        logger.info(f"AsyncBSLIndexer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"  Batch size: {batch_size}")
        logger.info(f"  Max workers: {max_workers}")
        logger.info(f"  Retry attempts: {retry_attempts}")

    async def index_directory_async(
        self,
        directory: str,
        max_files: Optional[int] = None,
        file_pattern: str = "*.bsl"
    ) -> int:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å BSL —Ñ–∞–π–ª–∞–º–∏

        Args:
            directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ (None = –≤—Å–µ)
            file_pattern: –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {directory}")

        # –ü–æ–∏—Å–∫ BSL —Ñ–∞–π–ª–æ–≤
        dir_path = Path(directory)
        if not dir_path.exists():
            logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
            return 0

        bsl_files = list(dir_path.rglob(file_pattern))
        self.progress.total_files = len(bsl_files)

        if max_files:
            bsl_files = bsl_files[:max_files]
            self.progress.total_files = len(bsl_files)
            logger.info(f"‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_files} —Ñ–∞–π–ª–æ–≤ –∏–∑ {len(list(dir_path.rglob(file_pattern)))}")

        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.progress.total_files}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.progress.start_time = time.time()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
        batches = [
            bsl_files[i:i + self.batch_size]
            for i in range(0, len(bsl_files), self.batch_size)
        ]

        logger.info(f"üì¶ –°–æ–∑–¥–∞–Ω–æ –±–∞—Ç—á–µ–π: {len(batches)}")

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            tasks = []
            for batch_idx, batch in enumerate(batches, 1):
                task = asyncio.create_task(
                    self._process_batch_async(batch, batch_idx, len(batches), executor)
                )
                tasks.append(task)

            # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
            await asyncio.gather(*tasks)

        # Retry –¥–ª—è —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
        if self.failed_files and self.retry_attempts > 0:
            logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(self.failed_files)} —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏...")
            await self._retry_failed_files()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._print_final_statistics()

        logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø–µ—à–Ω–æ: {self.progress.successful}/{self.progress.total_files}")
        return self.progress.successful

    async def _process_batch_async(
        self,
        batch: List[Path],
        batch_idx: int,
        total_batches: int,
        executor: ThreadPoolExecutor
    ):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞

        Args:
            batch: –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –±–∞—Ç—á–µ
            batch_idx: –ù–æ–º–µ—Ä –±–∞—Ç—á–∞
            total_batches: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π
            executor: Thread pool executor
        """
        logger.info(f"üì¶ –ë–∞—Ç—á {batch_idx}/{total_batches}: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batch)} —Ñ–∞–π–ª–æ–≤...")

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –±–∞—Ç—á–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(executor, self._index_file_sync, str(file_path))
            for file_path in batch
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –ü–æ–¥—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for result in results:
            self.progress.processed_files += 1

            if isinstance(result, Exception):
                self.progress.failed += 1
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_idx}: {result}")
            elif result:
                self.progress.successful += 1
            else:
                self.progress.skipped += 1

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ—Å–ª–µ –±–∞—Ç—á–∞
        self._print_progress()

    def _index_file_sync(self, file_path: str) -> bool:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–ª—è executor)

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
        """
        start_time = time.time()

        for attempt in range(1, self.retry_attempts + 1):
            try:
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞
                module = self.parser.parse_file(file_path)
                if not module:
                    logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å: {Path(file_path).name}")
                    return False

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
                searchable_text = self.parser.extract_searchable_text(module)

                # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                embedding = self.embedding_service.create_embedding(searchable_text)
                if not embedding:
                    logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥: {Path(file_path).name}")
                    return False

                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
                file_size = Path(file_path).stat().st_size

                # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                processing_time = (time.time() - start_time) * 1000  # ms

                # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∏–Ω–¥–µ–∫—Å–∞
                indexed_file = IndexedFile(
                    file_path=file_path,
                    module_type=module.module_type,
                    functions_count=len(module.functions),
                    variables_count=len(module.variables),
                    searchable_text=searchable_text,
                    embedding=embedding,
                    indexed_at=datetime.now().isoformat(),
                    file_size=file_size,
                    processing_time_ms=processing_time
                )

                self.indexed_files.append(indexed_file)
                return True

            except Exception as e:
                if attempt < self.retry_attempts:
                    logger.warning(f"‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{self.retry_attempts} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {Path(file_path).name}: {e}")
                    time.sleep(0.1 * attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {self.retry_attempts} –ø–æ–ø—ã—Ç–æ–∫ {Path(file_path).name}: {e}")
                    self.failed_files.append(file_path)
                    return False

        return False

    async def _retry_failed_files(self):
        """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏"""
        files_to_retry = self.failed_files.copy()
        self.failed_files.clear()

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            loop = asyncio.get_event_loop()
            tasks = [
                loop.run_in_executor(executor, self._index_file_sync, file_path)
                for file_path in files_to_retry
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            retry_success = sum(1 for r in results if r is True)
            logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —É—Å–ø–µ—à–Ω–æ {retry_success}/{len(files_to_retry)}")

    def _print_progress(self):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        progress_bar = self._create_progress_bar(self.progress.progress_percent)

        logger.info(
            f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_bar} {self.progress.progress_percent:.1f}%\n"
            f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.progress.processed_files}/{self.progress.total_files}\n"
            f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {self.progress.successful}\n"
            f"   ‚ùå –û—à–∏–±–æ–∫: {self.progress.failed}\n"
            f"   ‚è±Ô∏è  –°–∫–æ—Ä–æ—Å—Ç—å: {self.progress.files_per_second:.1f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫\n"
            f"   ‚è≥ –û—Å—Ç–∞–ª–æ—Å—å: ~{self.progress.estimated_remaining_seconds:.0f} —Å–µ–∫"
        )

    def _print_final_statistics(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        total_time = self.progress.elapsed_time

        logger.info(
            f"\n{'='*60}\n"
            f"üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n"
            f"{'='*60}\n"
            f"‚úÖ –£—Å–ø–µ—à–Ω–æ:        {self.progress.successful}\n"
            f"‚ùå –û—à–∏–±–æ–∫:         {self.progress.failed}\n"
            f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ:      {self.progress.skipped}\n"
            f"üìÅ –í—Å–µ–≥–æ:          {self.progress.total_files}\n"
            f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è:    {total_time:.1f} —Å–µ–∫ ({total_time/60:.1f} –º–∏–Ω)\n"
            f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {self.progress.files_per_second:.2f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫\n"
            f"{'='*60}"
        )

    @staticmethod
    def _create_progress_bar(percent: float, length: int = 30) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–π progress bar"""
        filled = int(length * percent / 100)
        bar = '‚ñà' * filled + '‚ñë' * (length - filled)
        return f"[{bar}]"

    def save_index(self, filename: str = "bsl_index_full.json"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ —Ñ–∞–π–ª

        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        output_path = self.output_dir / filename

        try:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –º–æ–¥—É–ª–µ–π
            module_types = {}
            for f in self.indexed_files:
                module_types[f.module_type] = module_types.get(f.module_type, 0) + 1

            # –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
            avg_processing_time = sum(f.processing_time_ms for f in self.indexed_files) / len(self.indexed_files) if self.indexed_files else 0

            index_data = {
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "total_files": len(self.indexed_files),
                    "embedding_model": self.embedding_service.model,
                    "embedding_dimension": len(self.indexed_files[0].embedding) if self.indexed_files else 0,
                    "batch_size": self.batch_size,
                    "max_workers": self.max_workers,
                    "total_processing_time_sec": self.progress.elapsed_time,
                    "avg_processing_time_ms": avg_processing_time,
                    "module_types": module_types,
                    "indexing_stats": {
                        "successful": self.progress.successful,
                        "failed": self.progress.failed,
                        "skipped": self.progress.skipped,
                        "total": self.progress.total_files
                    }
                },
                "files": [asdict(f) for f in self.indexed_files]
            }

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(index_data, f, ensure_ascii=False, indent=2)

            file_size_mb = output_path.stat().st_size / 1024 / 1024
            logger.info(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            logger.info(f"üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} MB")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if not self.indexed_files:
            return {"total_files": 0}

        total_functions = sum(f.functions_count for f in self.indexed_files)
        total_variables = sum(f.variables_count for f in self.indexed_files)
        total_size = sum(f.file_size for f in self.indexed_files)
        avg_processing_time = sum(f.processing_time_ms for f in self.indexed_files) / len(self.indexed_files)

        module_types = {}
        for f in self.indexed_files:
            module_types[f.module_type] = module_types.get(f.module_type, 0) + 1

        return {
            "total_files": len(self.indexed_files),
            "total_functions": total_functions,
            "total_variables": total_variables,
            "total_size_mb": total_size / 1024 / 1024,
            "module_types": module_types,
            "embedding_model": self.embedding_service.model,
            "embedding_dimension": len(self.indexed_files[0].embedding) if self.indexed_files else 0,
            "avg_processing_time_ms": avg_processing_time,
            "total_processing_time_sec": self.progress.elapsed_time,
            "files_per_second": self.progress.files_per_second,
            "indexing_stats": {
                "successful": self.progress.successful,
                "failed": self.progress.failed,
                "skipped": self.progress.skipped
            }
        }


# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
async def main_async():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse

    parser = argparse.ArgumentParser(description="Async BSL Indexer - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è BSL —Ñ–∞–π–ª–æ–≤")
    parser.add_argument(
        "directory",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å BSL —Ñ–∞–π–ª–∞–º–∏"
    )
    parser.add_argument(
        "--max-files",
        type=int,
        default=None,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ (default: –≤—Å–µ —Ñ–∞–π–ª—ã)"
    )
    parser.add_argument(
        "--output",
        default="D:/1C-Enterprise_Framework/ai-memory-system/data/index",
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=10,
        help="–†–∞–∑–º–µ—Ä batch –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (default: 10)"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ worker threads (default: 4)"
    )
    parser.add_argument(
        "--retry-attempts",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ (default: 3)"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞
    indexer = AsyncBSLIndexer(
        output_dir=args.output,
        batch_size=args.batch_size,
        max_workers=args.max_workers,
        retry_attempts=args.retry_attempts
    )

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    success_count = await indexer.index_directory_async(
        args.directory,
        max_files=args.max_files
    )

    if success_count > 0:
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        indexer.save_index()

        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = indexer.get_statistics()
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–î–ï–ö–°–ê–¶–ò–ò")
        print(f"{'='*60}")
        print(f"üìÅ –§–∞–π–ª–æ–≤:              {stats['total_files']}")
        print(f"üîß –§—É–Ω–∫—Ü–∏–π:             {stats['total_functions']}")
        print(f"üì¶ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:          {stats['total_variables']}")
        print(f"üíæ –†–∞–∑–º–µ—Ä:              {stats['total_size_mb']:.2f} MB")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å:              {stats['embedding_model']}")
        print(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å:         {stats['embedding_dimension']}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:     {stats['total_processing_time_sec']:.1f} —Å–µ–∫")
        print(f"‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å:     {stats['avg_processing_time_ms']:.1f} ms/—Ñ–∞–π–ª")
        print(f"üìà –§–∞–π–ª–æ–≤/—Å–µ–∫:          {stats['files_per_second']:.2f}")
        print(f"\nüìä –¢–∏–ø—ã –º–æ–¥—É–ª–µ–π:")
        for module_type, count in stats['module_types'].items():
            print(f"   {module_type}: {count}")
        print(f"{'='*60}\n")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    asyncio.run(main_async())


if __name__ == "__main__":
    main()
