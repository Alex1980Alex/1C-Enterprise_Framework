"""
Full Dataset Indexer with Resume Capability
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö BSL —Ñ–∞–π–ª–æ–≤ –≤ Qdrant –∏ Neo4j
"""

import sys
import json
import time
import asyncio
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Set
from dataclasses import dataclass, asdict
import logging

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π
sys.path.insert(0, str(Path(__file__).parent.parent))

from qdrant_client import QdrantClient
from services.embedding_service import EmbeddingService
from services.bsl_parser import BSLParser
from services.neo4j_indexer import Neo4jIndexer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('indexing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class IndexingProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    total_files: int
    processed_files: int
    failed_files: List[str]
    qdrant_indexed: Set[str]
    neo4j_indexed: Set[str]
    start_time: float
    last_checkpoint: float

    def to_dict(self) -> Dict:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è JSON"""
        return {
            'total_files': self.total_files,
            'processed_files': self.processed_files,
            'failed_files': self.failed_files,
            'qdrant_indexed': list(self.qdrant_indexed),
            'neo4j_indexed': list(self.neo4j_indexed),
            'start_time': self.start_time,
            'last_checkpoint': self.last_checkpoint
        }

    @classmethod
    def from_dict(cls, data: Dict) -> 'IndexingProgress':
        """–î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ JSON"""
        return cls(
            total_files=data['total_files'],
            processed_files=data['processed_files'],
            failed_files=data['failed_files'],
            qdrant_indexed=set(data['qdrant_indexed']),
            neo4j_indexed=set(data['neo4j_indexed']),
            start_time=data['start_time'],
            last_checkpoint=data['last_checkpoint']
        )


class FullDatasetIndexer:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π indexer —Å resume capability

    –£–ª—É—á—à–µ–Ω–∏—è:
    - –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout –¥–ª—è Ollama (90 —Å–µ–∫)
    - Resume capability —á–µ—Ä–µ–∑ checkpoints
    - –ü—Ä–æ–ø—É—Å–∫ —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    - Retry logic —Å exponential backoff
    - Real-time progress monitoring
    - –ü–æ–Ω–∏–∂–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ (batch=5)
    """

    def __init__(
        self,
        source_path: str,
        checkpoint_file: str = "data/indexing_progress.json",
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        ollama_host: str = "http://localhost:11434",
        neo4j_uri: str = "bolt://localhost:7687",
        neo4j_user: str = "neo4j",
        neo4j_password: str = "password123",
        batch_size: int = 5,
        max_retries: int = 3,
        ollama_timeout: int = 90
    ):
        self.source_path = Path(source_path)
        self.checkpoint_file = Path(checkpoint_file)
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.ollama_timeout = ollama_timeout

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è checkpoints
        self.checkpoint_file.parent.mkdir(parents=True, exist_ok=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤...")

        self.qdrant = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.embedding_service = EmbeddingService(
            ollama_host=ollama_host,
            model="nomic-embed-text:latest",
            timeout=ollama_timeout  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout
        )
        self.parser = BSLParser()
        self.neo4j = Neo4jIndexer(
            uri=neo4j_uri,
            user=neo4j_user,
            password=neo4j_password
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        self.progress = self._load_progress()

        logger.info(f"‚úÖ –°–µ—Ä–≤–∏—Å—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã (Ollama timeout: {ollama_timeout}s)")

    def _load_progress(self) -> IndexingProgress:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–∑ checkpoint"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                progress = IndexingProgress.from_dict(data)
                logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω checkpoint: {progress.processed_files}/{progress.total_files} —Ñ–∞–π–ª–æ–≤")
                return progress
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å checkpoint: {e}")

        # –ù–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        all_files = list(self.source_path.rglob("*.bsl"))
        return IndexingProgress(
            total_files=len(all_files),
            processed_files=0,
            failed_files=[],
            qdrant_indexed=set(),
            neo4j_indexed=set(),
            start_time=time.time(),
            last_checkpoint=time.time()
        )

    def _save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ checkpoint"""
        self.progress.last_checkpoint = time.time()
        try:
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress.to_dict(), f, indent=2, ensure_ascii=False)
            logger.debug(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint: {e}")

    def _get_pending_files(self) -> List[Path]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        all_files = list(self.source_path.rglob("*.bsl"))

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
        pending = []
        for file in all_files:
            file_str = str(file)
            if file_str not in self.progress.qdrant_indexed or file_str not in self.progress.neo4j_indexed:
                pending.append(file)

        return pending

    async def _index_file_to_qdrant(self, file_path: Path, retry_count: int = 0) -> bool:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –≤ Qdrant —Å retry logic

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥
            metadata = self.parser.parse_file(str(file_path))

            if not metadata:
                logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path.name}")
                return True

            # –°–æ–∑–¥–∞–Ω–∏–µ searchable text
            searchable_text = self.parser.create_searchable_text(metadata)

            # Embedding —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º timeout
            embedding = self.embedding_service.create_embedding(searchable_text)

            if not embedding:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å embedding –¥–ª—è {file_path.name}")
                return False

            # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
            point_id = hash(str(file_path)) & 0x7FFFFFFF  # Positive int

            self.qdrant.upsert(
                collection_name="bsl_code",
                points=[{
                    "id": point_id,
                    "vector": embedding,
                    "payload": {
                        "file_path": str(file_path),
                        "module_type": metadata.get('module_type', 'Unknown'),
                        "functions_count": len(metadata.get('functions', [])),
                        "procedures_count": len(metadata.get('procedures', [])),
                        "variables_count": len(metadata.get('variables', [])),
                        "searchable_text": searchable_text[:500]  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                    }
                }]
            )

            return True

        except Exception as e:
            if retry_count < self.max_retries:
                # Exponential backoff
                wait_time = 2 ** retry_count
                logger.warning(f"Retry {retry_count + 1}/{self.max_retries} –¥–ª—è {file_path.name} —á–µ—Ä–µ–∑ {wait_time}s")
                await asyncio.sleep(wait_time)
                return await self._index_file_to_qdrant(file_path, retry_count + 1)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Qdrant {file_path.name}: {e}")
                return False

    async def _index_file_to_neo4j(self, file_path: Path, retry_count: int = 0) -> bool:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –≤ Neo4j —Å retry logic

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        try:
            # –ü–∞—Ä—Å–∏–Ω–≥
            metadata = self.parser.parse_file(str(file_path))

            if not metadata:
                return True

            # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Neo4j
            self.neo4j.index_module(metadata)

            return True

        except Exception as e:
            if retry_count < self.max_retries:
                wait_time = 2 ** retry_count
                logger.warning(f"Retry {retry_count + 1}/{self.max_retries} –¥–ª—è {file_path.name} (Neo4j) —á–µ—Ä–µ–∑ {wait_time}s")
                await asyncio.sleep(wait_time)
                return await self._index_file_to_neo4j(file_path, retry_count + 1)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ Neo4j {file_path.name}: {e}")
                return False

    async def _process_batch(self, batch: List[Path]) -> Dict[str, int]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Ñ–∞–π–ª–æ–≤"""
        stats = {
            'qdrant_success': 0,
            'qdrant_failed': 0,
            'neo4j_success': 0,
            'neo4j_failed': 0
        }

        for file_path in batch:
            file_str = str(file_path)

            # Qdrant indexing
            if file_str not in self.progress.qdrant_indexed:
                success = await self._index_file_to_qdrant(file_path)
                if success:
                    self.progress.qdrant_indexed.add(file_str)
                    stats['qdrant_success'] += 1
                else:
                    stats['qdrant_failed'] += 1
                    if file_str not in self.progress.failed_files:
                        self.progress.failed_files.append(file_str)

            # Neo4j indexing
            if file_str not in self.progress.neo4j_indexed:
                success = await self._index_file_to_neo4j(file_path)
                if success:
                    self.progress.neo4j_indexed.add(file_str)
                    stats['neo4j_success'] += 1
                else:
                    stats['neo4j_failed'] += 1
                    if file_str not in self.progress.failed_files:
                        self.progress.failed_files.append(file_str)

            self.progress.processed_files += 1

        return stats

    def _print_progress(self, batch_stats: Dict[str, int]):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        elapsed = time.time() - self.progress.start_time
        processed = self.progress.processed_files
        total = self.progress.total_files

        if processed > 0:
            rate = processed / elapsed
            eta = (total - processed) / rate if rate > 0 else 0
        else:
            rate = 0
            eta = 0

        qdrant_total = len(self.progress.qdrant_indexed)
        neo4j_total = len(self.progress.neo4j_indexed)

        logger.info(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ïë –ü—Ä–æ–≥—Ä–µ—Å—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {processed}/{total} —Ñ–∞–π–ª–æ–≤ ({processed/total*100:.1f}%)
‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ïë Qdrant:  {qdrant_total} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ (‚úÖ {batch_stats['qdrant_success']} / ‚ùå {batch_stats['qdrant_failed']})
‚ïë Neo4j:   {neo4j_total} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ (‚úÖ {batch_stats['neo4j_success']} / ‚ùå {batch_stats['neo4j_failed']})
‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ïë –°–∫–æ—Ä–æ—Å—Ç—å: {rate:.2f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫
‚ïë –í—Ä–µ–º—è:    {elapsed/60:.1f} –º–∏–Ω—É—Ç
‚ïë ETA:      {eta/60:.1f} –º–∏–Ω—É—Ç
‚ïë Failed:   {len(self.progress.failed_files)} —Ñ–∞–π–ª–æ–≤
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """)

    async def run(self):
        """–ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        logger.info("üöÄ –°—Ç–∞—Ä—Ç –ø–æ–ª–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ BSL –∫–æ–¥–∞")
        logger.info(f"üìä –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {self.progress.total_files}")

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        pending_files = self._get_pending_files()
        logger.info(f"üìù –û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(pending_files)} —Ñ–∞–π–ª–æ–≤")

        if not pending_files:
            logger.info("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã!")
            return

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
        total_batches = (len(pending_files) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(pending_files), self.batch_size):
            batch = pending_files[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            logger.info(f"\nüì¶ –ë–∞—Ç—á {batch_num}/{total_batches} ({len(batch)} —Ñ–∞–π–ª–æ–≤)")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
            batch_stats = await self._process_batch(batch)

            # –í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            self._print_progress(batch_stats)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
            if batch_num % 10 == 0:
                self._save_progress()
                logger.info("üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

            # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            await asyncio.sleep(0.5)

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_progress()

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = time.time() - self.progress.start_time
        logger.info(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ïë ‚úÖ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê
‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ïë –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:        {self.progress.total_files}
‚ïë Qdrant indexed:      {len(self.progress.qdrant_indexed)}
‚ïë Neo4j indexed:       {len(self.progress.neo4j_indexed)}
‚ïë Failed:              {len(self.progress.failed_files)}
‚ïë ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ïë –í—Ä–µ–º—è:               {total_time/60:.1f} –º–∏–Ω—É—Ç
‚ïë –°–∫–æ—Ä–æ—Å—Ç—å:            {self.progress.processed_files/total_time:.2f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """)

        if self.progress.failed_files:
            logger.warning(f"\n‚ö†Ô∏è  Failed —Ñ–∞–π–ª—ã ({len(self.progress.failed_files)}):")
            for failed in self.progress.failed_files[:10]:
                logger.warning(f"  - {failed}")
            if len(self.progress.failed_files) > 10:
                logger.warning(f"  ... –∏ –µ—â—ë {len(self.progress.failed_files) - 10}")

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if hasattr(self, 'neo4j'):
            self.neo4j.close()
        logger.info("–°–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∑–∞–∫—Ä—ã—Ç—ã")


async def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Full Dataset Indexer for BSL code")
    parser.add_argument(
        "--source",
        default="D:/1C-Enterprise_Framework/src",
        help="–ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–∏–∫–∞–º BSL"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=5,
        help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (default: 5)"
    )
    parser.add_argument(
        "--ollama-timeout",
        type=int,
        default=90,
        help="Timeout –¥–ª—è Ollama –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 90)"
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=3,
        help="–ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ (default: 3)"
    )

    args = parser.parse_args()

    indexer = FullDatasetIndexer(
        source_path=args.source,
        batch_size=args.batch_size,
        ollama_timeout=args.ollama_timeout,
        max_retries=args.max_retries
    )

    try:
        await indexer.run()
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...")
        indexer._save_progress()
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        indexer._save_progress()
    finally:
        indexer.close()


if __name__ == "__main__":
    asyncio.run(main())
