"""
Load BSL Index to Qdrant - Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° BSL Ð¸Ð½Ð´ÐµÐºÑÐ° Ð² Qdrant
Ð’ÐµÑ€ÑÐ¸Ñ: 2.0 Ð´Ð»Ñ Week 2, Day 3

Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ:
- Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° Ð¸Ð· JSON Ð² Qdrant
- Batch processing Ð´Ð»Ñ ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸
- Progress monitoring
- Error handling
- Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ collection Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import time

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QdrantIndexLoader:
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸Ðº BSL Ð¸Ð½Ð´ÐµÐºÑÐ° Ð² Qdrant
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "bsl_code",
        batch_size: int = 100
    ):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸ÐºÐ°

        Args:
            qdrant_url: URL Qdrant ÑÐµÑ€Ð²ÐµÑ€Ð°
            collection_name: Ð˜Ð¼Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            batch_size: Ð Ð°Ð·Ð¼ÐµÑ€ batch Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        """
        self.client = QdrantClient(url=qdrant_url)
        self.collection_name = collection_name
        self.batch_size = batch_size

        logger.info(f"QdrantIndexLoader Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
        logger.info(f"  Qdrant URL: {qdrant_url}")
        logger.info(f"  Collection: {collection_name}")
        logger.info(f"  Batch size: {batch_size}")

    def load_index_file(self, index_file: str) -> Dict[str, Any]:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° Ð¸Ð· JSON Ñ„Ð°Ð¹Ð»Ð°

        Args:
            index_file: ÐŸÑƒÑ‚ÑŒ Ðº JSON Ñ„Ð°Ð¹Ð»Ñƒ

        Returns:
            Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°
        """
        logger.info(f"ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°: {index_file}")

        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)

            total_files = len(index_data.get('files', []))
            logger.info(f"âœ… Ð˜Ð½Ð´ÐµÐºÑ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½: {total_files} Ñ„Ð°Ð¹Ð»Ð¾Ð²")

            return index_data

        except FileNotFoundError:
            logger.error(f"âŒ Ð¤Ð°Ð¹Ð» Ð¸Ð½Ð´ÐµÐºÑÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {index_file}")
            return {}
        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¸Ð½Ð´ÐµÐºÑÐ°: {e}")
            return {}

    def create_collection(self, vector_size: int):
        """
        Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð² Qdrant

        Args:
            vector_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²
        """
        logger.info(f"ðŸ”§ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸: {self.collection_name}")

        try:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]

            if self.collection_name in collection_names:
                logger.warning(f"âš ï¸  ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚: {self.collection_name}")
                logger.info(f"ðŸ—‘ï¸  Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ€Ð¾Ð¹ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸...")
                self.client.delete_collection(collection_name=self.collection_name)

            # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð½Ð¾Ð²Ð¾Ð¹ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )

            logger.info(f"âœ… ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð°: {self.collection_name}")
            logger.info(f"   Ð Ð°Ð·Ð¼ÐµÑ€ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²: {vector_size}")
            logger.info(f"   ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°: COSINE")

        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸: {e}")
            raise

    def upload_to_qdrant(self, index_data: Dict[str, Any]) -> int:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ° Ð² Qdrant

        Args:
            index_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð½Ð´ÐµÐºÑÐ°

        Returns:
            ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ñ‚Ð¾Ñ‡ÐµÐº
        """
        metadata = index_data.get('metadata', {})
        files = index_data.get('files', [])

        if not files:
            logger.error("âŒ ÐÐµÑ‚ Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")
            return 0

        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
        vector_size = metadata.get('embedding_dimension', len(files[0]['embedding']))
        self.create_collection(vector_size)

        logger.info(f"ðŸ“¤ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² Qdrant...")
        logger.info(f"ðŸ“Š Ð’ÑÐµÐ³Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(files)}")

        # Ð Ð°Ð·Ð±Ð¸ÐµÐ½Ð¸Ðµ Ð½Ð° Ð±Ð°Ñ‚Ñ‡Ð¸
        batches = [
            files[i:i + self.batch_size]
            for i in range(0, len(files), self.batch_size)
        ]

        logger.info(f"ðŸ“¦ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹: {len(batches)}")

        total_uploaded = 0
        start_time = time.time()

        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸
        for batch_idx, batch in enumerate(batches, 1):
            try:
                points = []

                for file_idx, file_data in enumerate(batch):
                    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð´Ð»Ñ Qdrant
                    point = PointStruct(
                        id=total_uploaded + file_idx,
                        vector=file_data['embedding'],
                        payload={
                            'file_path': file_data['file_path'],
                            'module_type': file_data['module_type'],
                            'functions_count': file_data['functions_count'],
                            'variables_count': file_data['variables_count'],
                            'searchable_text': file_data['searchable_text'],
                            'file_size': file_data['file_size'],
                            'indexed_at': file_data['indexed_at'],
                            'processing_time_ms': file_data.get('processing_time_ms', 0)
                        }
                    )
                    points.append(point)

                # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð±Ð°Ñ‚Ñ‡Ð°
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=points
                )

                total_uploaded += len(points)

                # ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ
                progress = (batch_idx / len(batches)) * 100
                elapsed = time.time() - start_time
                speed = total_uploaded / elapsed if elapsed > 0 else 0

                logger.info(
                    f"ðŸ“¦ Ð‘Ð°Ñ‚Ñ‡ {batch_idx}/{len(batches)} ({progress:.1f}%): "
                    f"{len(points)} Ñ‚Ð¾Ñ‡ÐµÐº Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ | "
                    f"Ð’ÑÐµÐ³Ð¾: {total_uploaded} | "
                    f"Ð¡ÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ: {speed:.1f} Ñ‚Ð¾Ñ‡ÐµÐº/ÑÐµÐº"
                )

            except Exception as e:
                logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð±Ð°Ñ‚Ñ‡Ð° {batch_idx}: {e}")

        # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        total_time = time.time() - start_time
        logger.info(
            f"\n{'='*60}\n"
            f"âœ… Ð—ÐÐ“Ð Ð£Ð—ÐšÐ Ð—ÐÐ’Ð•Ð Ð¨Ð•ÐÐ\n"
            f"{'='*60}\n"
            f"ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ñ‚Ð¾Ñ‡ÐµÐº:   {total_uploaded}\n"
            f"â±ï¸  Ð’Ñ€ÐµÐ¼Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸:    {total_time:.1f} ÑÐµÐº\n"
            f"âš¡ Ð¡Ñ€ÐµÐ´Ð½ÑÑ ÑÐºÐ¾Ñ€Ð¾ÑÑ‚ÑŒ:  {total_uploaded/total_time:.1f} Ñ‚Ð¾Ñ‡ÐµÐº/ÑÐµÐº\n"
            f"{'='*60}"
        )

        return total_uploaded

    def verify_collection(self) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸

        Returns:
            True ÐµÑÐ»Ð¸ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð°
        """
        try:
            collection_info = self.client.get_collection(
                collection_name=self.collection_name
            )

            logger.info(
                f"\n{'='*60}\n"
                f"ðŸ” Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ Ðž ÐšÐžÐ›Ð›Ð•ÐšÐ¦Ð˜Ð˜\n"
                f"{'='*60}\n"
                f"ðŸ“ ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ:          {collection_info.config.params.vectors.size}\n"
                f"ðŸ“Š ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‚Ð¾Ñ‡ÐµÐº:  {collection_info.points_count}\n"
                f"ðŸ“ Ð Ð°Ð·Ð¼ÐµÑ€ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð²:   {collection_info.config.params.vectors.size}\n"
                f"ðŸ“ ÐœÐµÑ‚Ñ€Ð¸ÐºÐ°:           {collection_info.config.params.vectors.distance}\n"
                f"ðŸ’¾ Ð¡Ñ‚Ð°Ñ‚ÑƒÑ:            OK\n"
                f"{'='*60}"
            )

            return True

        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸: {e}")
            return False

    def test_search(self, query: str = "Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð±Ð°Ð·Ñ‹", limit: int = 5):
        """
        Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸

        Args:
            query: ÐŸÐ¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ
            limit: ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        """
        logger.info(f"\nðŸ” Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: '{query}'")

        try:
            # Ð”Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð½ÑƒÐ¶ÐµÐ½ embedding, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð²ÐµÐºÑ‚Ð¾Ñ€ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸
            # Ð’ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ embedding Ñ‡ÐµÑ€ÐµÐ· EmbeddingService
            logger.warning("âš ï¸  Ð”Ð»Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ EmbeddingService")
            logger.info("ðŸ’¡ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ qdrant_search.py Ð´Ð»Ñ ÑÐµÐ¼Ð°Ð½Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°")

        except Exception as e:
            logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ°: {e}")


def main():
    """Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ"""
    import argparse

    parser = argparse.ArgumentParser(description="Load BSL Index to Qdrant")
    parser.add_argument(
        "--index-file",
        default="D:/1C-Enterprise_Framework/ai-memory-system/data/index/bsl_index_full.json",
        help="ÐŸÑƒÑ‚ÑŒ Ðº JSON Ñ„Ð°Ð¹Ð»Ñƒ Ð¸Ð½Ð´ÐµÐºÑÐ°"
    )
    parser.add_argument(
        "--qdrant-url",
        default="http://localhost:6333",
        help="URL Qdrant ÑÐµÑ€Ð²ÐµÑ€Ð°"
    )
    parser.add_argument(
        "--collection",
        default="bsl_code",
        help="Ð˜Ð¼Ñ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ Ð² Qdrant"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Ð Ð°Ð·Ð¼ÐµÑ€ batch Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸"
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ Ð¿Ð¾ÑÐ»Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸"
    )

    args = parser.parse_args()

    # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·Ñ‡Ð¸ÐºÐ°
    loader = QdrantIndexLoader(
        qdrant_url=args.qdrant_url,
        collection_name=args.collection,
        batch_size=args.batch_size
    )

    # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¸Ð½Ð´ÐµÐºÑÐ°
    index_data = loader.load_index_file(args.index_file)

    if index_data:
        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð² Qdrant
        uploaded = loader.upload_to_qdrant(index_data)

        if uploaded > 0 and args.verify:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸
            loader.verify_collection()


if __name__ == "__main__":
    main()
