"""
–ú–∏–≥—Ä–∞—Ü–∏—è BSL –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ JSON –≤ Qdrant
–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –≤—Å–µ embeddings –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
"""

import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Batch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QdrantMigrator:
    """
    –ú–∏–≥—Ä–∞—Ü–∏—è BSL –∏–Ω–¥–µ–∫—Å–∞ –≤ Qdrant
    """

    def __init__(
        self,
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        collection_name: str = "bsl_code"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ç–æ—Ä–∞

        Args:
            qdrant_host: –•–æ—Å—Ç Qdrant
            qdrant_port: –ü–æ—Ä—Ç Qdrant
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        """
        self.client = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.collection_name = collection_name

        logger.info(f"QdrantMigrator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {qdrant_host}:{qdrant_port}")

    def load_json_index(self, json_path: str) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ JSON –∏–Ω–¥–µ–∫—Å–∞

        Args:
            json_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É

        Returns:
            –î–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                index_data = json.load(f)

            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {json_path}")
            logger.info(f"   –§–∞–π–ª–æ–≤: {len(index_data.get('files', []))}")

            return index_data

        except FileNotFoundError:
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_path}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None

    def create_collection(self, vector_size: int = 768, recreate: bool = False):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant

        Args:
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤
            recreate: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
            collections = self.client.get_collections().collections
            exists = any(c.name == self.collection_name for c in collections)

            if exists:
                if recreate:
                    logger.info(f"‚ôªÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{self.collection_name}'...")
                    self.client.delete_collection(self.collection_name)
                else:
                    logger.info(f"‚ö†Ô∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    return True

            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )

            logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —Å–æ–∑–¥–∞–Ω–∞")
            logger.info(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {vector_size}")
            logger.info(f"   –ú–µ—Ç—Ä–∏–∫–∞: COSINE")

            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            return False

    def migrate_batch(
        self,
        files: List[Dict[str, Any]],
        batch_size: int = 100
    ) -> int:
        """
        –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö batch-–∞–º–∏

        Args:
            files: –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –∏–∑ JSON –∏–Ω–¥–µ–∫—Å–∞
            batch_size: –†–∞–∑–º–µ—Ä batch –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫
        """
        total = len(files)
        migrated = 0

        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –º–∏–≥—Ä–∞—Ü–∏–∏: {total} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"   Batch size: {batch_size}")

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        with tqdm(total=total, desc="–ú–∏–≥—Ä–∞—Ü–∏—è", unit="—Ñ–∞–π–ª") as pbar:
            for i in range(0, total, batch_size):
                batch_files = files[i:i + batch_size]

                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ—á–µ–∫
                points = []
                for j, file_data in enumerate(batch_files):
                    point_id = i + j + 1  # ID –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 1

                    # Payload —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                    payload = {
                        "file_path": file_data.get("file_path", ""),
                        "module_type": file_data.get("module_type", "Unknown"),
                        "functions_count": file_data.get("functions_count", 0),
                        "variables_count": file_data.get("variables_count", 0),
                        "searchable_text": file_data.get("searchable_text", "")[:500],  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è payload
                        "file_size": file_data.get("file_size", 0),
                        "indexed_at": file_data.get("indexed_at", "")
                    }

                    # –í–µ–∫—Ç–æ—Ä
                    embedding = file_data.get("embedding", [])

                    if len(embedding) != 768:
                        logger.warning(f"‚ö†Ô∏è –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(embedding)}")
                        continue

                    point = PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload=payload
                    )

                    points.append(point)

                # –í—Å—Ç–∞–≤–∫–∞ batch
                try:
                    self.client.upsert(
                        collection_name=self.collection_name,
                        points=points,
                        wait=True
                    )

                    migrated += len(points)
                    pbar.update(len(batch_files))

                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ batch: {e}")

        logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {migrated}/{total} —Ñ–∞–π–ª–æ–≤")
        return migrated

    def verify_migration(self, expected_count: int):
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∏–≥—Ä–∞—Ü–∏–∏

        Args:
            expected_count: –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            actual_count = collection_info.points_count

            logger.info(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–∏–≥—Ä–∞—Ü–∏–∏:")
            logger.info(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: {expected_count}")
            logger.info(f"   –§–∞–∫—Ç–∏—á–µ—Å–∫–∏: {actual_count}")
            logger.info(f"   –£—Å–ø–µ—Ö: {actual_count == expected_count}")

            if actual_count == expected_count:
                logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫")

            return actual_count == expected_count

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return False

    def test_search(self, query_text: str = "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –∑–∞–ø–∏—Å–∏"):
        """
        –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã

        Args:
            query_text: –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        """
        try:
            logger.info(f"\nüîç –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: '{query_text}'")

            # –î–ª—è —Ç–µ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä
            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å embedding —á–µ—Ä–µ–∑ Ollama
            test_vector = [0.1] * 768

            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=test_vector,
                limit=3
            )

            logger.info(f"   –ù–∞–π–¥–µ–Ω–æ: {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

            for i, result in enumerate(results, 1):
                logger.info(f"\n   {i}. {Path(result.payload['file_path']).name}")
                logger.info(f"      Score: {result.score:.4f}")
                logger.info(f"      Type: {result.payload['module_type']}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏"""
    import argparse

    parser = argparse.ArgumentParser(description="–ú–∏–≥—Ä–∞—Ü–∏—è BSL –∏–Ω–¥–µ–∫—Å–∞ –≤ Qdrant")
    parser.add_argument(
        "--json",
        default="D:/1C-Enterprise_Framework/ai-memory-system/data/index/bsl_index.json",
        help="–ü—É—Ç—å –∫ JSON –∏–Ω–¥–µ–∫—Å—É"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="–†–∞–∑–º–µ—Ä batch –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ (default: 100)"
    )
    parser.add_argument(
        "--recreate",
        action="store_true",
        help="–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é"
    )

    args = parser.parse_args()

    print("=" * 70)
    print("üì¶ –ú–∏–≥—Ä–∞—Ü–∏—è BSL –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ JSON –≤ Qdrant")
    print("=" * 70)

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–∏–≥—Ä–∞—Ç–æ—Ä–∞
    migrator = QdrantMigrator()

    # –ó–∞–≥—Ä—É–∑–∫–∞ JSON –∏–Ω–¥–µ–∫—Å–∞
    print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ JSON –∏–Ω–¥–µ–∫—Å–∞...")
    index_data = migrator.load_json_index(args.json)

    if not index_data:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å")
        return

    files = index_data.get("files", [])
    metadata = index_data.get("metadata", {})

    print(f"   –§–∞–π–ª–æ–≤: {len(files)}")
    print(f"   –ú–æ–¥–µ–ª—å: {metadata.get('embedding_model', 'unknown')}")
    print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {metadata.get('embedding_dimension', 0)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    print("\n2Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant...")
    vector_size = metadata.get("embedding_dimension", 768)

    if not migrator.create_collection(vector_size, recreate=args.recreate):
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é")
        return

    # –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\n3Ô∏è‚É£ –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    migrated_count = migrator.migrate_batch(files, batch_size=args.batch_size)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n4Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    success = migrator.verify_migration(len(files))

    if success:
        print("\n" + "=" * 70)
        print("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"   –ú–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ: {migrated_count} —Ñ–∞–π–ª–æ–≤")
        print("=" * 70)
    else:
        print("\n" + "=" * 70)
        print("‚ö†Ô∏è –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
        print("=" * 70)


if __name__ == "__main__":
    main()
