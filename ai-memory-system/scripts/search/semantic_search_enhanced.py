"""
Enhanced Semantic Search –¥–ª—è BSL –∫–æ–¥–∞
–í–µ—Ä—Å–∏—è: 2.0 —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- Semantic search —á–µ—Ä–µ–∑ Qdrant
- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (vector + metadata filters)
- Ranking –∏ scoring
- Highlighted —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- Export –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
"""

import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from services.embedding_service import EmbeddingService

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    file_path: str
    module_type: str
    score: float
    functions_count: int
    variables_count: int
    preview: str
    file_size: int
    indexed_at: str

    @property
    def relevance_label(self) -> str:
        """–ú–µ—Ç–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self.score >= 0.8:
            return "–û—Ç–ª–∏—á–Ω–æ"
        elif self.score >= 0.6:
            return "–•–æ—Ä–æ—à–æ"
        elif self.score >= 0.4:
            return "–°—Ä–µ–¥–Ω–µ"
        else:
            return "–°–ª–∞–±–æ"


class SemanticSearchEngine:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è BSL –∫–æ–¥–∞
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "bsl_code",
        embedding_model: str = "nomic-embed-text:latest"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞

        Args:
            qdrant_url: URL Qdrant —Å–µ—Ä–≤–µ—Ä–∞
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è embeddings
        """
        self.client = QdrantClient(url=qdrant_url)
        self.collection_name = collection_name
        self.embedding_service = EmbeddingService(model=embedding_model)

        logger.info(f"SemanticSearchEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        logger.info(f"  Qdrant: {qdrant_url}")
        logger.info(f"  Collection: {collection_name}")
        logger.info(f"  Model: {embedding_model}")

    def search(
        self,
        query: str,
        limit: int = 10,
        min_score: float = 0.3,
        module_type: Optional[str] = None,
        min_functions: Optional[int] = None
    ) -> List[SearchResult]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            module_type: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –º–æ–¥—É–ª—è
            min_functions: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫: '{query}'")

        # –°–æ–∑–¥–∞–Ω–∏–µ embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_vector = self.embedding_service.create_embedding(query)
        if not query_vector:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
            return []

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        search_filter = None
        filter_conditions = []

        if module_type:
            filter_conditions.append(
                FieldCondition(
                    key="module_type",
                    match=MatchValue(value=module_type)
                )
            )

        if min_functions is not None:
            filter_conditions.append(
                FieldCondition(
                    key="functions_count",
                    range={
                        "gte": min_functions
                    }
                )
            )

        if filter_conditions:
            search_filter = Filter(must=filter_conditions)

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
        try:
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_vector,
                limit=limit * 2,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ score
                query_filter=search_filter
            )

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É score
            filtered_results = [
                r for r in results
                if r.score >= min_score
            ][:limit]

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ SearchResult
            search_results = []
            for result in filtered_results:
                payload = result.payload

                search_result = SearchResult(
                    file_path=payload['file_path'],
                    module_type=payload['module_type'],
                    score=result.score,
                    functions_count=payload['functions_count'],
                    variables_count=payload['variables_count'],
                    preview=payload['searchable_text'][:300],
                    file_size=payload['file_size'],
                    indexed_at=payload['indexed_at']
                )
                search_results.append(search_result)

            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(search_results)}")
            return search_results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def search_similar_code(
        self,
        code_snippet: str,
        limit: int = 5
    ) -> List[SearchResult]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–¥–∞

        Args:
            code_snippet: –§—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∞–π–ª–æ–≤
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–¥–∞ (–¥–ª–∏–Ω–∞: {len(code_snippet)} —Å–∏–º–≤–æ–ª–æ–≤)")

        return self.search(
            query=code_snippet,
            limit=limit,
            min_score=0.5  # –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è code similarity
        )

    def get_statistics(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        try:
            collection_info = self.client.get_collection(
                collection_name=self.collection_name
            )

            stats = {
                "collection_name": self.collection_name,
                "total_points": collection_info.points_count,
                "vector_size": collection_info.config.params.vectors.size,
                "distance_metric": str(collection_info.config.params.vectors.distance),
                "status": "OK"
            }

            return stats

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

    def export_results(
        self,
        results: List[SearchResult],
        format: str = "json",
        output_file: Optional[str] = None
    ) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ (json, csv, markdown)
            output_file: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞

        Returns:
            –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        if format == "json":
            data = json.dumps(
                [asdict(r) for r in results],
                ensure_ascii=False,
                indent=2
            )

        elif format == "csv":
            import csv
            from io import StringIO

            output = StringIO()
            writer = csv.DictWriter(
                output,
                fieldnames=['file_path', 'module_type', 'score', 'relevance_label', 'functions_count']
            )
            writer.writeheader()

            for r in results:
                writer.writerow({
                    'file_path': r.file_path,
                    'module_type': r.module_type,
                    'score': f"{r.score:.3f}",
                    'relevance_label': r.relevance_label,
                    'functions_count': r.functions_count
                })

            data = output.getvalue()

        elif format == "markdown":
            lines = ["# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞\n"]

            for i, r in enumerate(results, 1):
                lines.append(f"## {i}. {Path(r.file_path).name}")
                lines.append(f"**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å**: {r.score:.3f} ({r.relevance_label})")
                lines.append(f"**–¢–∏–ø**: {r.module_type}")
                lines.append(f"**–§—É–Ω–∫—Ü–∏–π**: {r.functions_count}")
                lines.append(f"**–ü—É—Ç—å**: `{r.file_path}`")
                lines.append(f"\n**–ü—Ä–µ–≤—å—é**:\n```bsl\n{r.preview}\n```\n")

            data = "\n".join(lines)

        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(data)
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {output_file}")

        return data

    def print_results(self, results: List[SearchResult], detailed: bool = False):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å

        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            detailed: –ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        """
        if not results:
            print("\n‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return

        print(f"\n{'='*80}")
        print(f"üîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–ò–°–ö–ê ({len(results)} –Ω–∞–π–¥–µ–Ω–æ)")
        print(f"{'='*80}\n")

        for i, result in enumerate(results, 1):
            score_bar = self._create_score_bar(result.score)

            print(f"{i}. {Path(result.file_path).name}")
            print(f"   üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score_bar} {result.score:.3f} ({result.relevance_label})")
            print(f"   üìÅ –¢–∏–ø: {result.module_type}")
            print(f"   üîß –§—É–Ω–∫—Ü–∏–π: {result.functions_count} | –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {result.variables_count}")

            if detailed:
                print(f"   üìÇ –ü—É—Ç—å: {result.file_path}")
                print(f"   üíæ –†–∞–∑–º–µ—Ä: {result.file_size} bytes")
                print(f"   üìÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {result.indexed_at}")
                print(f"\n   üìù –ü—Ä–µ–≤—å—é:\n   {result.preview[:200]}...\n")
            else:
                print(f"   üìù {result.preview[:100]}...\n")

        print(f"{'='*80}\n")

    @staticmethod
    def _create_score_bar(score: float, length: int = 20) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —à–∫–∞–ª—ã score"""
        filled = int(length * score)
        bar = '‚ñà' * filled + '‚ñë' * (length - filled)
        return f"[{bar}]"


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Enhanced Semantic Search –¥–ª—è BSL –∫–æ–¥–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
  python semantic_search_enhanced.py "–ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã"

  # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
  python semantic_search_enhanced.py "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞" --module-type ObjectModule --limit 5

  # –ü–æ–∏—Å–∫ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ—É–Ω–∫—Ü–∏–π
  python semantic_search_enhanced.py "—Ä–∞–±–æ—Ç–∞ —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏" --min-functions 3

  # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
  python semantic_search_enhanced.py "–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ" --export json --output results.json

  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
  python semantic_search_enhanced.py --stats
        """
    )

    parser.add_argument(
        "query",
        nargs='?',
        help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (default: 10)"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=0.3,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score (0.0-1.0, default: 0.3)"
    )
    parser.add_argument(
        "--module-type",
        help="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –º–æ–¥—É–ª—è"
    )
    parser.add_argument(
        "--min-functions",
        type=int,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–π"
    )
    parser.add_argument(
        "--detailed",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
    )
    parser.add_argument(
        "--export",
        choices=['json', 'csv', 'markdown'],
        help="–§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    )
    parser.add_argument(
        "--output",
        help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–ª–ª–µ–∫—Ü–∏–∏"
    )
    parser.add_argument(
        "--qdrant-url",
        default="http://localhost:6333",
        help="URL Qdrant —Å–µ—Ä–≤–µ—Ä–∞"
    )
    parser.add_argument(
        "--collection",
        default="bsl_code",
        help="–ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    engine = SemanticSearchEngine(
        qdrant_url=args.qdrant_url,
        collection_name=args.collection
    )

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    if args.stats:
        stats = engine.get_statistics()
        print(f"\n{'='*60}")
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–õ–õ–ï–ö–¶–ò–ò")
        print(f"{'='*60}")
        print(f"–ù–∞–∑–≤–∞–Ω–∏–µ:      {stats.get('collection_name', 'N/A')}")
        print(f"–¢–æ—á–µ–∫:         {stats.get('total_points', 0):,}")
        print(f"–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('vector_size', 0)}")
        print(f"–ú–µ—Ç—Ä–∏–∫–∞:       {stats.get('distance_metric', 'N/A')}")
        print(f"–°—Ç–∞—Ç—É—Å:        {stats.get('status', 'N/A')}")
        print(f"{'='*60}\n")
        return

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–ø—Ä–æ—Å–∞
    if not args.query:
        parser.print_help()
        return

    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–∞
    results = engine.search(
        query=args.query,
        limit=args.limit,
        min_score=args.min_score,
        module_type=args.module_type,
        min_functions=args.min_functions
    )

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    engine.print_results(results, detailed=args.detailed)

    # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if args.export and results:
        output_file = args.output or f"search_results.{args.export}"
        engine.export_results(results, format=args.export, output_file=output_file)


if __name__ == "__main__":
    main()
