"""
ContextRestoration Service

–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç ConversationStorage –∏ MessageVectorization –¥–ª—è
–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏.
"""

from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import logging

from conversation_storage import ConversationStorage
from message_vectorization import MessageVectorization

logger = logging.getLogger(__name__)


class ContextRestoration:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤

    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è context summary –¥–ª—è Claude
    - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    """

    def __init__(
        self,
        storage: ConversationStorage,
        vectorizer: MessageVectorization
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞

        Args:
            storage: –≠–∫–∑–µ–º–ø–ª—è—Ä ConversationStorage
            vectorizer: –≠–∫–∑–µ–º–ø–ª—è—Ä MessageVectorization
        """
        self.storage = storage
        self.vectorizer = vectorizer
        logger.info("ContextRestoration initialized")

    def get_relevant_context(
        self,
        query: Optional[str] = None,
        project_context: Optional[str] = None,
        session_id: Optional[str] = None,
        max_messages: int = 20,
        include_recent: bool = True,
        include_semantic: bool = True,
        min_importance: float = 0.5
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–∏

        Args:
            query: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            project_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É)
            session_id: ID –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
            max_messages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            include_recent: –í–∫–ª—é—á–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            include_semantic: –í–∫–ª—é—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ
            min_importance: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–π

        Returns:
            Dict —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        context = {
            "recent_conversations": [],
            "recent_messages": [],
            "semantic_matches": [],
            "important_messages": [],
            "context_summary": "",
            "total_messages": 0
        }

        try:
            # 1. Get recent conversations for the project
            if project_context:
                recent_convs = self.storage.get_recent_conversations(
                    limit=5,
                    project_context=project_context,
                    status='active'
                )
                context["recent_conversations"] = recent_convs
                logger.info(f"Found {len(recent_convs)} recent conversations for {project_context}")

            # 2. Get recent messages (temporal context)
            if include_recent:
                recent_msgs = self.storage.get_important_messages(
                    limit=max_messages // 2,
                    min_score=min_importance,
                    project_context=project_context
                )
                context["recent_messages"] = recent_msgs
                logger.info(f"Found {len(recent_msgs)} recent messages")

            # 3. Get semantically similar messages
            if include_semantic and query:
                semantic_msgs = self.vectorizer.search_similar_messages(
                    query=query,
                    limit=max_messages // 2,
                    min_score=0.6
                )
                context["semantic_matches"] = semantic_msgs
                logger.info(f"Found {len(semantic_msgs)} semantic matches for query")

            # 4. Get important messages
            important_msgs = self.storage.get_important_messages(
                limit=10,
                min_score=0.7,
                project_context=project_context
            )
            context["important_messages"] = important_msgs

            # 5. Build context summary
            context["context_summary"] = self._build_context_summary(context)
            context["total_messages"] = (
                len(context["recent_messages"]) +
                len(context["semantic_matches"]) +
                len(context["important_messages"])
            )

            logger.info(f"Restored context with {context['total_messages']} total messages")
            return context

        except Exception as e:
            logger.error(f"Failed to get relevant context: {e}")
            raise

    def _build_context_summary(self, context: Dict) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            context: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

        Returns:
            –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è injection –≤ Claude
        """
        summary_parts = []

        # Recent conversations summary
        if context["recent_conversations"]:
            summary_parts.append("=== Recent Conversations ===")
            for conv in context["recent_conversations"][:3]:
                summary_parts.append(
                    f"- Session: {conv['session_id']} "
                    f"({conv['total_messages']} messages, "
                    f"avg importance: {conv.get('avg_importance', 0):.2f})"
                )

        # Important messages summary
        if context["important_messages"]:
            summary_parts.append("\n=== Important Messages ===")
            for msg in context["important_messages"][:5]:
                role_marker = "üë§" if msg["role"] == "user" else "ü§ñ"
                summary_parts.append(
                    f"{role_marker} [{msg['importance_score']:.2f}] "
                    f"{msg['content_preview'][:100]}..."
                )

        # Semantic matches summary
        if context["semantic_matches"]:
            summary_parts.append("\n=== Semantically Related ===")
            for match in context["semantic_matches"][:3]:
                summary_parts.append(
                    f"[Score: {match['score']:.2f}] "
                    f"{match['content_preview'][:100]}..."
                )

        # Recent messages summary
        if context["recent_messages"]:
            summary_parts.append("\n=== Recent Activity ===")
            for msg in context["recent_messages"][:5]:
                role = "User" if msg["role"] == "user" else "Assistant"
                summary_parts.append(
                    f"- {role}: {msg['content_preview'][:80]}..."
                )

        return "\n".join(summary_parts)

    def restore_conversation_context(
        self,
        conversation_id: str
    ) -> Dict[str, Any]:
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

        Args:
            conversation_id: UUID —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

        Returns:
            Dict —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        """
        try:
            # Get conversation metadata
            conversation = self.storage.get_conversation(conversation_id)
            if not conversation:
                logger.warning(f"Conversation {conversation_id} not found")
                return {}

            # Get all messages
            messages = self.storage.get_conversation_messages(
                conversation_id,
                limit=None
            )

            # Get vectorized context
            vector_context = self.vectorizer.get_conversation_context(
                conversation_id,
                limit=100
            )

            return {
                "conversation": conversation,
                "messages": messages,
                "message_count": len(messages),
                "vector_count": len(vector_context),
                "session_id": conversation["session_id"],
                "project_context": conversation.get("project_context"),
                "status": conversation["status"]
            }

        except Exception as e:
            logger.error(f"Failed to restore conversation context: {e}")
            raise

    def search_conversation_history(
        self,
        query: str,
        project_context: Optional[str] = None,
        days_back: int = 30,
        limit: int = 50
    ) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            project_context: –§–∏–ª—å—Ç—Ä –ø–æ –ø—Ä–æ–µ–∫—Ç—É
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–∞–¥
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        try:
            # Combine full-text search and semantic search
            fts_results = self.storage.search_messages_by_text(
                search_text=query,
                limit=limit // 2,
                project_context=project_context
            )

            semantic_results = self.vectorizer.search_similar_messages(
                query=query,
                limit=limit // 2,
                min_score=0.5
            )

            # Merge and deduplicate
            all_results = fts_results + [
                {
                    "message_id": r["message_id"],
                    "content_preview": r["content_preview"],
                    "score": r["score"],
                    "role": r["role"],
                    "source": "semantic"
                }
                for r in semantic_results
            ]

            # Remove duplicates by message_id
            seen_ids = set()
            unique_results = []
            for result in all_results:
                msg_id = result.get("message_id")
                if msg_id and msg_id not in seen_ids:
                    seen_ids.add(msg_id)
                    unique_results.append(result)

            logger.info(f"Search found {len(unique_results)} unique results for query: {query[:50]}...")
            return unique_results[:limit]

        except Exception as e:
            logger.error(f"Failed to search conversation history: {e}")
            raise

    def restore_context(
        self,
        conversation_id: str,
        query: Optional[str] = None,
        max_messages: int = 20
    ) -> List[Dict[str, Any]]:
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è MCP —Å–µ—Ä–≤–µ—Ä–æ–º)

        Args:
            conversation_id: UUID —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –∏–ª–∏ session_id
            query: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            max_messages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π

        Returns:
            List —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        try:
            # Try to get conversation messages
            messages = self.storage.get_conversation_messages(
                conversation_id,
                limit=max_messages
            )

            if not messages:
                logger.warning(f"No messages found for conversation {conversation_id}")
                return []

            # Format messages for MCP response
            formatted_messages = []
            for msg in messages:
                formatted_messages.append({
                    "role": msg.get("role", "unknown"),
                    "content": msg.get("content", ""),
                    "importance_score": msg.get("importance_score", 0.0),
                    "timestamp": msg.get("created_at", "").isoformat() if hasattr(msg.get("created_at", ""), "isoformat") else str(msg.get("created_at", ""))
                })

            logger.info(f"Restored {len(formatted_messages)} messages for conversation {conversation_id}")
            return formatted_messages

        except Exception as e:
            logger.error(f"Failed to restore context for conversation {conversation_id}: {e}")
            # Return empty list instead of raising to avoid breaking MCP server
            return []

    def get_project_summary(
        self,
        project_context: str,
        include_stats: bool = True
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –ø—Ä–æ–µ–∫—Ç—É

        Args:
            project_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
            include_stats: –í–∫–ª—é—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É

        Returns:
            Dict —Å–æ —Å–≤–æ–¥–∫–æ–π –ø–æ –ø—Ä–æ–µ–∫—Ç—É
        """
        try:
            # Get recent conversations
            conversations = self.storage.get_recent_conversations(
                limit=10,
                project_context=project_context,
                status='active'
            )

            # Get important messages
            important = self.storage.get_important_messages(
                limit=20,
                min_score=0.7,
                project_context=project_context
            )

            summary = {
                "project_context": project_context,
                "active_conversations": len(conversations),
                "important_messages": len(important),
                "total_messages": sum(c["total_messages"] for c in conversations)
            }

            if include_stats:
                stats = self.storage.get_stats()
                summary["global_stats"] = stats

            logger.info(f"Generated summary for project: {project_context}")
            return summary

        except Exception as e:
            logger.error(f"Failed to get project summary: {e}")
            raise


# Example usage
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # Database configuration
    DB_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'ai_memory',
        'user': 'ai_user',
        'password': 'ai_memory_secure_2025'
    }

    # Initialize services
    storage = ConversationStorage(DB_CONFIG)
    vectorizer = MessageVectorization(
        qdrant_host="localhost",
        qdrant_port=6333,
        collection_name="conversation_memory"
    )
    restoration = ContextRestoration(storage, vectorizer)

    # Test context restoration
    print("=== Testing Context Restoration ===\n")

    # Get relevant context
    context = restoration.get_relevant_context(
        query="–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ TimescaleDB",
        project_context="1C-Enterprise_Framework",
        max_messages=10
    )

    print(f"Total messages in context: {context['total_messages']}")
    print(f"Recent conversations: {len(context['recent_conversations'])}")
    print(f"Semantic matches: {len(context['semantic_matches'])}")
    print("\n=== Context Summary ===")
    print(context["context_summary"])

    # Search conversation history
    print("\n=== Searching History ===")
    results = restoration.search_conversation_history(
        query="Qdrant embedding",
        limit=5
    )
    print(f"Found {len(results)} results")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['content_preview'][:80]}...")

    # Get project summary
    print("\n=== Project Summary ===")
    summary = restoration.get_project_summary(
        project_context="1C-Enterprise_Framework"
    )
    print(f"Active conversations: {summary['active_conversations']}")
    print(f"Important messages: {summary['important_messages']}")
    print(f"Total messages: {summary['total_messages']}")
